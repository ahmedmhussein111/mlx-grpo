# ğŸ§  MLX-GRPO: Train Your Own DeepSeek-R1 on Mac

<div align="center">

![Apple Silicon](https://img.shields.io/badge/Apple_Silicon-Native-blue?style=for-the-badge&logo=apple)
![MLX](https://img.shields.io/badge/MLX-Optimized-orange?style=for-the-badge)
![GRPO](https://img.shields.io/badge/GRPO-DeepSeek_R1_Style-red?style=for-the-badge)
![MIT License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**ğŸ”¥ The FIRST MLX implementation of GRPO - Train reasoning models like DeepSeek-R1 ğŸ”¥**

*Build your own o1-style reasoning AI using the same technique that powers DeepSeek-R1*

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ§  What is GRPO?](#-what-is-grpo) â€¢ [âš¡ Performance](#-why-mlx--apple-silicon) â€¢ [ğŸ¯ Examples](#-training-examples)

</div>

---

## ğŸ¯ Why This Matters Right Now

**DeepSeek-R1 just shocked the AI world** by matching o1 performance using GRPO (Group Relative Policy Optimization). Now you can:

- ğŸ§  **Train o1-style reasoning models** - Same technique as DeepSeek-R1
- âš¡ **On your Mac** - Native Apple Silicon optimization via MLX
- ğŸ’° **No human feedback needed** - Programmable rewards instead of expensive RLHF
- ğŸ¯ **Multi-step reasoning** - Perfect for math, coding, and complex problems
- ğŸš€ **Production ready** - Robust checkpointing and speculative decoding

> *"GRPO is the technique behind DeepSeek-R1's breakthrough performance"* - Recent AI research shows GRPO enables direct optimization using programmable reward functions, making it more scalable than traditional RLHF approaches

## ğŸ§  What is GRPO?

**Group Relative Policy Optimization** is the secret sauce behind DeepSeek-R1's reasoning abilities:

- ğŸ“Š **Compares multiple responses** to the same question within each batch
- ğŸ¯ **Learns from relative quality** - promotes better answers, demotes worse ones  
- ğŸ”„ **Online learning** - improves iteratively using the model's own generated data
- ğŸ›ï¸ **Programmable rewards** - no need for expensive human preference data
- ğŸ§® **Perfect for reasoning** - excels at multi-step problems like math and coding

The GRPO update compares multiple answers to a single question within a batch, teaching the model to become more like correct answers and less like incorrect ones.

## ğŸš€ Quick Start

Get your GRPO reasoning model running in **3 minutes**:

```bash
# 1. Clone and install
git clone https://github.com/adeelahmad/mlx-grpo.git
cd mlx-grpo
pip install mlx mlx-lm numpy rich datasets

# 2. Train a math reasoning model (like DeepSeek-R1)
python mlx_grpo_trainer_aligned.py \
  --model_path microsoft/DialoGPT-medium \
  --train_dataset_path ./data/math_problems.jsonl \
  --reward_content_type math_eval \
  --num_training_steps 5000

# 3. Test your reasoning model
python test_reasoning.py --model ./output_model
```

**That's it!** ğŸ‰ You now have a reasoning model trained with the same technique as DeepSeek-R1.

## âš¡ Why MLX + Apple Silicon?

| Traditional Training | MLX-GRPO on Mac | Advantage |
|---------------------|-----------------|-----------|
| Requires expensive GPUs | Runs on any Mac with Apple Silicon | **ğŸ’° Cost savings** |
| Complex CUDA setup | Zero configuration needed | **ğŸš€ Easy setup** |
| High memory usage | MLX optimized memory management | **ğŸ“± Efficient** |
| Slow on consumer hardware | Native Apple Silicon acceleration | **âš¡ Fast training** |

*MLX is Apple's machine learning framework designed specifically for efficient training and inference on Apple Silicon.*

## ğŸ¯ Training Examples

### ğŸ§® **Mathematics Reasoning (DeepSeek-R1 style)**
```bash
python mlx_grpo_trainer_aligned.py \
  --model_path microsoft/DialoGPT-medium \
  --train_dataset_path ./data/math_qa.jsonl \
  --reward_content_type math_eval \
  --reward_format_weight 0.3 \
  --reward_content_weight 0.7 \
  --num_training_steps 8500
```
*Trains a model to show step-by-step mathematical reasoning*

### ğŸ’­ **Chain-of-Thought Reasoning**
```bash
python mlx_grpo_trainer_aligned.py \
  --model_path microsoft/DialoGPT-large \
  --train_dataset_path ./data/reasoning.jsonl \
  --reward_content_type jaccard \
  --num_training_steps 10000
```
*Optimizes for the `<think>...</think><answer>...</answer>` format used by o1 and R1*

### ğŸ¯ **Multiple Choice Questions**
```bash
python mlx_grpo_trainer_aligned.py \
  --dataset_name "your-mcq-dataset" \
  --reward_content_type choice_correctness \
  --num_training_steps 6000
```
*Perfect for training on standardized tests and benchmarks*

## ğŸ› ï¸ Advanced Features

### ğŸ¯ **Smart Reward System**
- **ğŸ“ Format Rewards**: Ensures proper `<think>...</think><answer>...</answer>` structure
- **ğŸ§® Math Evaluation**: Automatically checks mathematical correctness  
- **ğŸ“Š Jaccard Similarity**: Measures word overlap with reference answers
- **âœ… Choice Correctness**: Perfect for multiple-choice problems
- **ğŸ”§ Custom Rewards**: Build your own reward functions

### ğŸš€ **Production Features**
- **ğŸ’¾ Atomic Checkpointing**: Never lose training progress
- **âš¡ Speculative Decoding**: 2x faster inference with draft models
- **ğŸ¨ Rich CLI**: Beautiful progress bars and logging
- **ğŸ”„ Auto-Resume**: Continues exactly where you left off
- **ğŸ“Š Weights & Biases**: Optional experiment tracking

### ğŸ›ï¸ **Flexible Configuration**
```python
# All training parameters
@dataclass
class TrainingArgs:
    model_path: str = "../Model"
    output_dir: str = "../OutputModel" 
    num_training_steps: int = 8500
    reward_content_type: str = "jaccard"  # jaccard, math_eval, choice_correctness
    reward_format_weight: float = 0.5
    reward_content_weight: float = 0.5
    # ... and many more!
```

## ğŸ“Š Complete Configuration Options

<details>
<summary>ğŸ“‹ All Training Parameters</summary>

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--output_dir` | Directory for checkpoints and outputs | `../OutputModel` |
| `--model_path` | Path or ID of the base MLX model | `../Model` |
| `--train_dataset_path` | Local training JSONL file | `../dataset_512/train.jsonl` |
| `--val_dataset_path` | Local validation JSONL file | `../dataset_512/valid.jsonl` |
| `--num_training_steps` | Number of optimizer steps | `8500` |
| `--reward_content_type` | Content reward: `jaccard`, `math_eval`, `choice_correctness` | `jaccard` |
| `--reward_format_weight` | Weight for format reward (0.0 - 1.0) | `0.5` |
| `--reward_content_weight` | Weight for content reward (0.0 - 1.0) | `0.5` |

*See `TrainingArgs` dataclass in the code for the complete list*

</details>

## ğŸ”¥ What's Hot About This

### ğŸ¯ **Trending AI Techniques**
- âœ… **GRPO** - Same as DeepSeek-R1 (trending #1 on Twitter)
- âœ… **Chain-of-Thought** - o1-style reasoning format
- âœ… **Apple Silicon ML** - Fastest growing ML platform
- âœ… **Reward-Free RL** - No expensive human feedback needed

### ğŸš€ **Perfect Timing**
- ğŸ”¥ **DeepSeek-R1** just dominated benchmarks using GRPO
- ğŸ“ˆ **Apple MLX** adoption growing rapidly  
- ğŸ’¡ **Reasoning models** are the hottest topic in AI
- ğŸ’° **Cost-effective** alternative to GPT-4/Claude for reasoning

## ğŸ¤ Community & Support

<div align="center">

**Join the MLX + GRPO Revolution**

[![Issues](https://img.shields.io/github/issues/adeelahmad/mlx-grpo?style=for-the-badge)](https://github.com/adeelahmad/mlx-grpo/issues)
[![Pull Requests](https://img.shields.io/github/issues-pr/adeelahmad/mlx-grpo?style=for-the-badge)](https://github.com/adeelahmad/mlx-grpo/pulls)
[![Stars](https://img.shields.io/github/stars/adeelahmad/mlx-grpo?style=for-the-badge)](https://github.com/adeelahmad/mlx-grpo/stargazers)

</div>

### ğŸš€ Resources
- ğŸ“š [GRPO Explained](https://www.deeplearning.ai/short-courses/reinforcement-fine-tuning-llms-grpo/) - DeepLearning.AI Course
- ğŸ”¬ [DeepSeek-R1 Technical Report](https://arxiv.org/abs/2501.12948) - How they used GRPO
- ğŸ [MLX Documentation](https://github.com/ml-explore/mlx) - Apple's ML framework
- ğŸ’¬ [HuggingFace GRPO Guide](https://huggingface.co/docs/trl/main/en/grpo_trainer) - Alternative implementation

## ğŸ› ï¸ Requirements

- ğŸ **Apple Silicon Mac** (M1, M2, M3, M4) or any MLX-supported hardware
- ğŸ **Python â‰¥3.8**
- ğŸ“¦ **Dependencies**: `mlx`, `mlx-lm`, `numpy`, `rich`, `datasets`
- ğŸ’¾ **Optional**: `psutil`, `wandb` for enhanced monitoring

## ğŸ¤ Contributing

We â¤ï¸ contributions! This is a hot research area with lots of room for improvement:

1. ğŸ´ **Fork the repo**
2. ğŸŒ¿ **Create feature branch** (`git checkout -b amazing-feature`)
3. ğŸ’« **Commit changes** (`git commit -m 'Add amazing feature'`)
4. ğŸš€ **Push to branch** (`git push origin amazing-feature`)
5. ğŸ‰ **Open Pull Request**

### ğŸ¯ Contribution Ideas
- ğŸ”§ New reward functions for specific domains
- âš¡ Performance optimizations for MLX
- ğŸ“Š Better evaluation metrics
- ğŸ¨ Enhanced CLI visualization
- ğŸ“ More training examples and tutorials

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- ğŸ **Apple** for the incredible [MLX framework](https://github.com/ml-explore/mlx)
- ğŸ¤— **HuggingFace** for [MLX-LM](https://github.com/ml-explore/mlx-lm) and [datasets](https://github.com/huggingface/datasets)
- ğŸ¨ **Textualize** for the beautiful [Rich](https://github.com/Textualize/rich) library
- ğŸ§  **DeepSeek** for pioneering GRPO in their R1 model
- ğŸ”¬ **Research community** advancing reinforcement learning for LLMs

---

<div align="center">

**â­ Star us if you're excited about training reasoning models on Mac! â­**

*Built with ğŸ§  for the future of AI reasoning*

**ğŸ”¥ Trending:** `#GRPO` `#DeepSeekR1` `#MLX` `#AppleSilicon` `#ReasoningAI` `#MachineLearning`

</div>
